"""
Dataset module for loading and processing synthetic 3D scene data.

This module provides the SceneDataset class for loading scenes generated
by the Blender script, handling image loading, transformations, and
ground truth data extraction.
"""
import os
import json
import random
from PIL import Image
import torch

class SceneDataset:
    """
    A class to represent the synthetic dataset of scenes.
    Each item in the dataset corresponds to a full scene (all frames).
    """
    def __init__(self, data_dir, transform=None):
        """
        Initializes the dataset by locating all scenes.

        Args:
            data_dir (str): The path to the root directory containing the scene folders
                            (e.g., 'data/').
            transform (callable, optional): A function/transform that takes in a PIL image
                                            and returns a transformed version. E.g., a
                                            `torchvision.transforms.Compose` object.
        """
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []

        if not os.path.isdir(self.data_dir):
            raise FileNotFoundError(f"Dataset directory not found: {self.data_dir}")

        # Get all scene directories, sort them to ensure a consistent order
        scene_dirs = sorted([d for d in os.listdir(self.data_dir) if d.startswith('scene_')])

        for scene_name in scene_dirs:
            scene_path = os.path.join(self.data_dir, scene_name)
            json_path = os.path.join(scene_path, 'scene_data.json')

            if os.path.exists(json_path):
                # Each scene is now a single sample
                self.samples.append({
                    'scene_name': scene_name,
                    'json_path': json_path,
                })

    def __len__(self):
        """
        Returns the total number of scenes in the dataset.
        """
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Retrieves a single full scene from the dataset by its index.

        Args:
            idx (int): The index of the scene to retrieve.

        Returns:
            dict: A dictionary containing the stacked images, poses, and ground truth.
                  {
                      'images': torch.Tensor,   # Shape: [num_frames, C, H, W]
                      'poses': torch.Tensor,     # Shape: [num_frames, 6] (loc+rot)
                      'gt_location': torch.Tensor # Shape: [3]
                  }
        """
        sample_info = self.samples[idx]

        # Load the scene's ground truth data from the JSON file
        with open(sample_info['json_path'], 'r', encoding='utf-8') as f:
            scene_data = json.load(f)

        # --- Process all images in the scene ---
        images = []
        num_frames = 16  # As defined in the generation script
        for frame_idx in range(num_frames):
            image_path = os.path.join(self.data_dir, sample_info['scene_name'],
                                     f"frame_{frame_idx:02d}.png")
            image = Image.open(image_path).convert('RGB')

            if self.transform:
                image = self.transform(image)
            images.append(image)

        # Stack images into a single tensor
        images_tensor = torch.stack(images)

        # --- Process all camera poses ---
        camera_poses = []
        for pose_info in scene_data['camera_poses']:
            pose_vector = pose_info['location'] + pose_info['rotation']
            camera_poses.append(pose_vector)

        poses_tensor = torch.tensor(camera_poses, dtype=torch.float32)

        # --- Get ground truth location (of the first object) ---
        # The generation script ensures at least one object exists.
        gt_location = torch.tensor(scene_data['objects'][0]['location'], dtype=torch.float32)

        return {
            'images': images_tensor,
            'poses': poses_tensor,
            'gt_location': gt_location
        }

# --- Example Usage ---
if __name__ == '__main__':
    # Assume your data is in a folder named 'dat' as generated by the Blender script
    # Change this to 'data' if you have renamed it.
    DATA_DIRECTORY = 'data'

    # To run this example, you need PyTorch and Torchvision installed:
    # pip install torch torchvision
    try:
        from torchvision import transforms

        # Example of a standard transformation pipeline for image models
        image_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        print(f"Loading dataset from: {DATA_DIRECTORY} with transformations.")
        dataset = SceneDataset(data_dir=DATA_DIRECTORY, transform=image_transform)

        print("Dataset loaded successfully.")
        # The length is now the number of scenes
        print(f"Total number of scenes in the dataset: {len(dataset)}")

        # Get a random sample from the dataset
        random_index = random.randint(0, len(dataset) - 1)
        print(f"\nRetrieving scene at random index: {random_index}")

        sample = dataset[random_index]

        # Print the information for the retrieved sample
        print("--- Sample Data ---")
        print(f"Images tensor shape: {sample['images'].shape}")
        print(f"Poses tensor shape: {sample['poses'].shape}")
        print(f"Ground truth location tensor shape: {sample['gt_location'].shape}")
        print(f"Ground truth location: {sample['gt_location']}")

    except ImportError:
        print("PyTorch or Torchvision is not installed. Skipping example with transforms.")
        print("To run this example, install it with: pip install torch torchvision")
    except FileNotFoundError as e:
        print(e)
    except Exception as e:
        print(f"An error occurred: {e}")
